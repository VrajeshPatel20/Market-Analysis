{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyaxVQLKMHkR",
        "outputId": "25b8fc18-1e52-42cb-9482-fcaa85a6b661"
      },
      "source": [
        "###################### Install all necessary libraries #########################\n",
        "\n",
        "from google.colab import files\n",
        "!pip install praw -q\n",
        "!pip install yfinance --upgrade --no-cache-dir -q\n",
        "!pip install yahoo_fin -q\n",
        "!pip install yahoo_fin --upgrade -q\n",
        "!pip install requests_html -q\n",
        "!pip install mpld3 -q\n",
        "!pip install fastapi -q\n",
        "!pip install colabcode -q\n",
        "\n",
        "################################################################################"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 163kB 3.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3MB 4.2MB/s \n",
            "\u001b[?25h  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 3.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 3.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 3.2MB/s \n",
            "\u001b[?25h  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pyppeteer 0.2.5 has requirement urllib3<2.0.0,>=1.25.8, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 890kB 2.5MB/s \n",
            "\u001b[?25h  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.1MB 4.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 2.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 747kB 4.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.3MB 5.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 378kB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 430kB 40.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 122kB 38.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 6.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[?25h  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement tornado~=5.1.0; python_version >= \"3.0\", but you'll have tornado 6.1 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGr8DEs7MPNl"
      },
      "source": [
        "#########################################  DATA REQUESTS #############################################\n",
        "\n",
        "# To provide current date and time in an format understandable by our API\n",
        "from datetime import datetime \n",
        "from datetime import timedelta\n",
        "# Properly display html docs before we could work on it\n",
        "from lxml import html \n",
        "# Requests HTML data from websites and api \n",
        "import requests \n",
        "from requests import Request, Session\n",
        "\n",
        "# Connecting with an API without any exceptions\n",
        "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects \n",
        "\n",
        "# Clean the result from API's\n",
        "import json  \n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "##########################################  DATA MINING ##############################################\n",
        "\n",
        "# To get posts from reddit\n",
        "import praw\n",
        "\n",
        "# To get the current stock prices\n",
        "import yfinance as yf  \n",
        "from yahoo_fin.stock_info import get_data\n",
        "\n",
        "# Connect to twitter API\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import tweepy\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "# Library to set up email alerts \n",
        "import smtplib\n",
        "\n",
        "#######################################  DATA MANIPULATION ###########################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import os\n",
        "import time\n",
        "\n",
        "#######################################  DATA VISUALIZATION ###########################################\n",
        " \n",
        "import plotly.express as px \n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pylab as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
        "import mpld3\n",
        "import plotly\n",
        "\n",
        "########################################### DATA TRANSFER ############################################\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from colabcode import ColabCode \n",
        "\n",
        "#######################################################################################################"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qUESJ-nMRrN"
      },
      "source": [
        "def Reddit_API(client_id, client_secret, username, password, subreddit, limit):\n",
        "    \"\"\"\n",
        "    Receive the content of ``subreddit`` , establish credentials and retreive posts \n",
        "    parse the data by iterating over the list.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    client_id     : str\n",
        "    client_secret : str\n",
        "    username      : str\n",
        "    password      : str\n",
        "    subreddit     : str\n",
        "    limit         : int\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    #################### Establishing Credentials for Reddit ###################\n",
        "\n",
        "    client_auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
        "    data = pd.DataFrame()\n",
        "    params = {'limit': limit}\n",
        "    data = {\n",
        "        'grant_type': 'password',\n",
        "        'username'  : username,\n",
        "        'password'  : password\n",
        "    }\n",
        "    headers = {'User-Agent': 'data_analysis'}\n",
        "\n",
        "    reddit = requests.post('https://www.reddit.com/api/v1/access_token',\n",
        "                    auth=client_auth, data=data, headers=headers)\n",
        "\n",
        "    \n",
        "    ############################# Retrieving Data ##############################\n",
        "    \n",
        "    token = f\"bearer {reddit.json()['access_token']}\"\n",
        "    headers = {**headers, **{'Authorization': token}}\n",
        "\n",
        "    res_new = requests.get(\"https://oauth.reddit.com/r/\"+subreddit+\"/new\",\n",
        "                       headers=headers, params=params)\n",
        "    res_top = requests.get(\"https://oauth.reddit.com/r/\"+subreddit+\"/top\",\n",
        "                       headers=headers, params=params)\n",
        "    res_hot = requests.get(\"https://oauth.reddit.com/r/\"+subreddit+\"/hot\",\n",
        "                       headers=headers, params=params)\n",
        "    res_rising = requests.get(\"https://oauth.reddit.com/r/\"+subreddit+\"/rising\",\n",
        "                       headers=headers, params=params)\n",
        "    res_rec = requests.get(\"https://oauth.reddit.com/r/\"+subreddit,\n",
        "                       headers=headers, params=params)\n",
        "    \n",
        "    ############################### Parsing Data ###############################\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    posts = res_new.json()['data']['children']\n",
        "\n",
        "    for post in posts:\n",
        "      if post['data']['selftext']:\n",
        "        df = df.append({'Title'       : post['data']['title'],\n",
        "                        'Content'     : post['data']['selftext'],\n",
        "                        'upvote_ratio': post['data']['upvote_ratio'],\n",
        "                        'Upvotes'     : post['data']['ups'],\n",
        "                        'score'       : post['data']['score'],\n",
        "                        'type'        : post['data']['link_flair_css_class']\n",
        "        }, ignore_index=True)\n",
        "    \n",
        "    posts = res_top.json()['data']['children']\n",
        "    for post in posts:\n",
        "      if post['data']['selftext']:\n",
        "        df = df.append({'Title'       : post['data']['title'],\n",
        "                        'Content'     : post['data']['selftext'],\n",
        "                        'upvote_ratio': post['data']['upvote_ratio'],\n",
        "                        'Upvotes'     : post['data']['ups'],\n",
        "                        'score'       : post['data']['score'],\n",
        "                        'type'        : post['data']['link_flair_css_class']\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    posts = res_hot.json()['data']['children']\n",
        "    for post in posts:\n",
        "      if post['data']['selftext']:\n",
        "        df = df.append({'Title'       : post['data']['title'],\n",
        "                        'Content'     : post['data']['selftext'],\n",
        "                        'upvote_ratio': post['data']['upvote_ratio'],\n",
        "                        'Upvotes'     : post['data']['ups'],\n",
        "                        'score'       : post['data']['score'],\n",
        "                        'type'        : post['data']['link_flair_css_class']\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    posts = res_rising.json()['data']['children']\n",
        "    for post in posts:\n",
        "      if post['data']['selftext']:\n",
        "        df = df.append({'Title'       : post['data']['title'],\n",
        "                        'Content'     : post['data']['selftext'],\n",
        "                        'upvote_ratio': post['data']['upvote_ratio'],\n",
        "                        'Upvotes'     : post['data']['ups'],\n",
        "                        'score'       : post['data']['score'],\n",
        "                        'type'        : post['data']['link_flair_css_class']\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    posts = res_rec.json()['data']['children']\n",
        "    for post in posts:\n",
        "      if post['data']['selftext']:\n",
        "        df = df.append({'Title'       : post['data']['title'],\n",
        "                        'Content'     : post['data']['selftext'],\n",
        "                        'upvote_ratio': post['data']['upvote_ratio'],\n",
        "                        'Upvotes'     : post['data']['ups'],\n",
        "                        'score'       : post['data']['score'],\n",
        "                        'type'        : post['data']['link_flair_css_class']\n",
        "        }, ignore_index=True)\n",
        "\n",
        "    ############################################################################\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yvQ0ygeMRpU"
      },
      "source": [
        "def getDataReddit(client_id, client_secret, username, password, user_agent, title, lt):\n",
        "    \"\"\"\n",
        "    Receive the content of ``title`` (subreddit), establish credentials and retreive posts \n",
        "    parse it using BeautifulSoup and return the DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    client_id     : str\n",
        "    client_secret : str\n",
        "    username      : str\n",
        "    password      : str\n",
        "    user_agent    : str\n",
        "    title         : str\n",
        "    lt            : int\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    #################### Establishing Credentials for Reddit ###################\n",
        "    \n",
        "    reddit = praw.Reddit(client_id = client_id, \n",
        "                         client_secret = client_secret, \n",
        "                         username = username, \n",
        "                         password = password, \n",
        "                         user_agent = user_agent)\n",
        "    \n",
        "    ############## Using Credentials established to retreive data ##############\n",
        "    \n",
        "    subreddit = reddit.subreddit(title) \n",
        "    raw_data = subreddit.new(limit = lt)\n",
        "    top_data = subreddit.top(\"week\")\n",
        "    \n",
        "    ######################## Parsing the collected data ########################\n",
        "    \n",
        "    data = []\n",
        "    \n",
        "    for submission in top_data:\n",
        "      try:\n",
        "        value = Sentiment(BeautifulSoup(submission.selftext_html,\"lxml\")).analyze()\n",
        "      except:\n",
        "        value = 0\n",
        "\n",
        "      data.append({'Title':submission.title,\n",
        "                  'Content':BeautifulSoup(submission.selftext_html,\"lxml\").text,\n",
        "                  'Upvotes':submission.ups,'Downvotes':submission.downs})\n",
        "    \n",
        "    for submission in raw_data:\n",
        "      data.append({'Title':submission.title,\n",
        "                  'Content':BeautifulSoup(submission.selftext_html,\"lxml\").text,\n",
        "                  'Upvotes':submission.ups,'Downvotes':submission.downs})\n",
        "    \n",
        "    ############################################################################ \n",
        "\n",
        "    return pd.DataFrame(data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoZW11hFMRmW"
      },
      "source": [
        "def getDataTwitter(consumer_key, consumer_secret, access_key, access_secret, hashtags, target_date, total_tweets, attempts):\n",
        "    \"\"\"\n",
        "    Receive the content of ``hashtags`` (tweets) by establishing credentials. \n",
        "    Parse the data we want to use and return the DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    consumer_key    : str\n",
        "    consumer_secret : str\n",
        "    access_key      : str\n",
        "    access_secret   : str\n",
        "    hashtags        : str\n",
        "    target_date     : str\n",
        "    total_tweets    : int\n",
        "    attempts        : int\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    ################### Establishing Credentials for Twitter ###################\n",
        "\n",
        "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    api = tweepy.API(auth)\n",
        "\n",
        "    ############################# Retrieving Data ##############################\n",
        "\n",
        "    # create an empty DataFrame to store tweets later\n",
        "    db_tweets = pd.DataFrame(columns = ['username', 'location', 'text', 'retweet_count'])\n",
        "    count = 0\n",
        "\n",
        "    # Collect all the tweets received in tweet_list for n attempts\n",
        "    for i in range(0, attempts):\n",
        "      tweets = tweepy.Cursor(api.search, q=hashtags, lang=\"en\", since=target_date, tweet_mode='extended').items(total_tweets)\n",
        "      tweet_list = [tweet for tweet in tweets]\n",
        "\n",
        "    ######################## Parsing the collected data ########################\n",
        "\n",
        "    for item in tweet_list:\n",
        "      username, location, retweet_count = item.user.screen_name, item.user.location, item.retweet_count\n",
        "      try:\n",
        "        text = item.retweeted_status.full_text\n",
        "      except AttributeError:\n",
        "        text = item.full_text\n",
        "      \n",
        "      # Arrange and store the data collected for the tweet\n",
        "      curr = [username, location, text, retweet_count]\n",
        "      \n",
        "      db_tweets.loc[len(db_tweets)] = curr\n",
        "      count += 1\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "    # Good Night zzzzz\n",
        "    time.sleep(900)\n",
        "    return db_tweets"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srl0JKeBMRjk"
      },
      "source": [
        "def clean(dataFrame):\n",
        "    \"\"\"\n",
        "    Simply dropping duplicates from the dataframe.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dataFrame : pd.DataFrame\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    dataFrame.drop_duplicates()\n",
        "    return dataFrame"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X8MPuJsMRg1"
      },
      "source": [
        "# Enter your credentials \n",
        "client_id, client_secret = 'jA6tGV9IbyEDxg', 'ku7t8VHg5vtp3JMGjuqJmE5ybQhBxA'\n",
        "username, password = '-betrayer', 'NarutoUzamaki$$$'\n",
        "\n",
        "# We are currently using the normal API to retreive data\n",
        "reddit_posts = pd.DataFrame()\n",
        "reddit_crypto = pd.DataFrame()\n",
        "subreddits_stocks = [\"wallstreetbets\", \"stocks\", \"investing\", \"securityanalysis\"]\n",
        "for subreddit in subreddits_stocks:\n",
        "  new_data = Reddit_API(client_id, client_secret, username, password, subreddit, 10000)\n",
        "  reddit_posts = reddit_posts.append(new_data)\n",
        "subreddits_crypto = [\"Bitcoinmarkets\", \"Ethfinance\", \"CryptoMarkets\", \"CryptoCurrencyTrading\", \"ethtrader\", \"Cryptocurrency\", \"Crypto_Currency_News\"]\n",
        "for subreddit in subreddits_crypto:\n",
        "  new_data = Reddit_API(client_id, client_secret, username, password, subreddit, 10000)\n",
        "  reddit_crypto = reddit_crypto.append(new_data)\n",
        "  \n",
        "# To Use the praw library Uncomment the code below\n",
        "# df = getDataReddit(client_id, client_secret, username, password, user_agent, 'stocks', 100) \n",
        "# df = clean(df)\n",
        "# reddit_posts = reddit_posts.append(df)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoGI2EraMReE",
        "outputId": "08f723e9-4363-446a-80b6-c40b3aac5ea4"
      },
      "source": [
        "# Size of our retrieved data\n",
        "len(reddit_posts)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "940"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkQkA2eXM5u8"
      },
      "source": [
        "consumer_key, consumer_secret = '02tFL0DX18rEZN43uV9iCWr6E', 'Bx3xDbD3a9fuP5Kv90ZGHD5amoJXAhF3l5sEElnp6H4esWDk1e'\n",
        "access_key, access_secret = '1189932019243003904-85tElnlmswS7Of3dkiIzdImn2J4xiA', '6xt9bsQxlxyBUnR2p71mYYbZ42nDJK7TWCQPeAtsIL5zj'\n",
        "# df_tweets_crypto = getDataTwitter(consumer_key, consumer_secret, access_key, access_secret, hashtags=\"#Dogecoin OR #DogecoinToTheMoon OR #Dogearmy OR #doge\", target_date='2021-04-30', total_tweets = 2500, attempts = 1)\n",
        "# df_tweets = getDataTwitter(consumer_key, consumer_secret, access_key, access_secret, hashtags=\"#stocks OR #StockToBuy OR #trading OR #stockmarket OR #investing\", target_date='2021-04-30', total_tweets = 2500, attempts = 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUDfImrBMRbM"
      },
      "source": [
        "def stocks():\n",
        "    \"\"\"\n",
        "    Receive the content of ``stock_dataset_url``, parse it using beautiful soup and return it as a DataFrame.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    ############################## Retrieving Data #############################\n",
        "\n",
        "    stock_dataset_url = 'https://stockanalysis.com/stocks/'\n",
        "    page = requests.get(stock_dataset_url)\n",
        "    soup = BeautifulSoup(page.text,'html.parser')\n",
        "    review = soup.find_all(class_='no-spacing')\n",
        "    review_stocks = review[0].find_all('a')\n",
        "    \n",
        "    ############################### Parsing Data ###############################\n",
        "    \n",
        "    stock_list = []\n",
        "    all_stocks = []\n",
        "    for item in review_stocks:\n",
        "      res = item.text.split('-')\n",
        "      stock_list.append({'Ticker':res[0],'Stock_Name':res[1]})\n",
        "      all_stocks.append(res[1])\n",
        "    \n",
        "    ############################################################################\n",
        "\n",
        "    return pd.DataFrame(stock_list)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSYQfhQRMRWs"
      },
      "source": [
        "# Get Top Gainers or Top Losers in the Stock Market Today\n",
        "def Gainers_Or_Losers(x):\n",
        "    \"\"\"\n",
        "    Receive the content of ``url``(Gainers if x == 1 else Losers), parse it as JSON and return the object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : int\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame \n",
        "    \"\"\"\n",
        "    ############################# Retrieving Data ##############################\n",
        "\n",
        "    url2 = (\"https://financialmodelingprep.com/api/v3/losers?apikey=71a14544ca8435ff9b1d2ad551cf5b4e#0\")\n",
        "    url1 = (\"https://financialmodelingprep.com/api/v3/stock/gainers?apikey=71a14544ca8435ff9b1d2ad551cf5b4e\")\n",
        "    response = urlopen(url1 if x==1 else url2)\n",
        "    data = response.read().decode(\"utf-8\")\n",
        "\n",
        "    ############################ Parsing Data(arr) #############################\n",
        "\n",
        "    value = json.loads(data)\n",
        "    try:\n",
        "      arr = [item for item in value['mostGainerStock']]\n",
        "    except:\n",
        "      arr = [item for item in value]\n",
        "    df = pd.DataFrame(arr)\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "    return df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhBRRLcuMjwP"
      },
      "source": [
        "def CoinBase_Api():\n",
        "    \"\"\"\n",
        "    Receive the content of ``url``, parse it as JSON and return the data necessary as pd.DataFrame.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame \n",
        "    \"\"\"\n",
        "    ############################# Retrieving Data ##############################\n",
        "\n",
        "    url = 'https://pro-api.coinmarketcap.com/v1/cryptocurrency/listings/latest'\n",
        "    parameters = {\n",
        "      'start':'1',\n",
        "      'limit':'5000',\n",
        "      'convert':'USD'\n",
        "    }\n",
        "    headers = {\n",
        "      'Accepts': 'application/json',\n",
        "      'X-CMC_PRO_API_KEY': 'e5078d40-9c0f-45f5-8e1c-42c06a61b3c4',\n",
        "    }\n",
        "\n",
        "    session = Session()\n",
        "    session.headers.update(headers)\n",
        "    \n",
        "    ############################# Parsing Data #################################\n",
        "\n",
        "    try:\n",
        "      response = session.get(url, params=parameters)\n",
        "      data = json.loads(response.text)\n",
        "      result = []\n",
        "      for i in data['data']:\n",
        "        result.append({'Name'              : i['name'],\n",
        "                       'Symbol'            : i['symbol'],\n",
        "                       'Price'             : i['quote']['USD']['price'],\n",
        "                       'percent_change_1h' : i['quote']['USD']['percent_change_1h'],\n",
        "                       'percent_change_24h': i['quote']['USD']['percent_change_24h'],\n",
        "                       'percent_change_7d' : i['quote']['USD']['percent_change_7d'],\n",
        "                       'percent_change_30d': i['quote']['USD']['percent_change_30d'],\n",
        "                       'volume_24'         : i['quote']['USD']['volume_24h'],\n",
        "                       'Trading_Volume'    : i['quote']['USD']['market_cap'],\n",
        "                       'Circulating_Supply': i['circulating_supply']})\n",
        "      return pd.DataFrame(result)\n",
        "    except (ConnectionError, Timeout, TooManyRedirects) as e:\n",
        "      return e\n",
        "\n",
        "    ############################################################################"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCMcqBc-MjtV"
      },
      "source": [
        "def email_formatter(test_data):\n",
        "    \"\"\"\n",
        "    Parse the DataFrame and convert it to a readable string \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    test_data : pd.DataFrame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "    \"\"\"\n",
        "    ############################ Formatting the Data ###########################\n",
        "    \n",
        "    count = 1\n",
        "\n",
        "    str_send = \"List of Cryptos with growth over 500% today. \\n\\n\\n\"\n",
        "    str_send += \"{:<8} {:<35} {:<15} {:<15} {:<15}\".format('No.', 'Name','Symbol','Price','Percent Change')\n",
        "    str_send += \"\\n\"\n",
        "\n",
        "    for index, row in test_data.iterrows():\n",
        "      price = '{:.7f}'.format(row['Price'])\n",
        "      str_send+=\"{:<8} {:<35} {:<15} {:<15} {:<15}\".format(count,row['Name'], row['Symbol'], price, row['percent_change_24h'])\n",
        "      str_send+=\"\\n\"\n",
        "      count+=1\n",
        "      \n",
        "    \n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "    return str_send"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcMXQ5GfMjq0"
      },
      "source": [
        "def Alerts(sender_email, sender_password, receiver_list):\n",
        "    \"\"\"\n",
        "    Using the CoinBase_Api() email the top performing crytos and \n",
        "    stocks to the receiver list provided. \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sender_email    : str\n",
        "    sender_password : str\n",
        "    receiver_list   : list\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    E-mail\n",
        "    \"\"\"\n",
        "    ############################ Collecting Data ###############################\n",
        "\n",
        "    crypto_data = CoinBase_Api()\n",
        "    stocks_gainers = Gainers_Or_Losers(1)\n",
        "    stocks_losers = Gainers_Or_Losers(0)\n",
        "    \n",
        "    # Get the cryptos where growth is over 500% and stocks where growth is over 10% in last 24h \n",
        "    test_data = crypto_data.sort_values('percent_change_24h', ascending=False)\n",
        "    test_data = test_data[test_data['percent_change_24h']>500]\n",
        "\n",
        "    # Check whether there is any data to post or not \n",
        "    to_post = len(test_data) > 0 \n",
        "\n",
        "    ################ E-mailing Results if Crypto growth > 500 ##################\n",
        "\n",
        "    if to_post:\n",
        "        # creates SMTP session\n",
        "        s = smtplib.SMTP('smtp.gmail.com', 587)\n",
        "\n",
        "        # start TLS for security\n",
        "        s.starttls()\n",
        "\n",
        "        # Authentication\n",
        "        s.login(sender_email, sender_password)\n",
        "\n",
        "        # Output Message\n",
        "        TEXT = email_formatter(test_data)\n",
        "        TEXT += \"\\n\\n The Top Stock Gainers in the market today are : \\n\" + stocks_gainers.to_string() +\"\\n\\n The top Stock Losers in the market today are : \\n\" + stocks_losers.to_string()\n",
        "        TEXT += \"\\n\\n Have a good day \\n Regards,\\n Vrajesh\" \n",
        "        output_message = 'Subject: {}\\n\\n{}'.format(\"Crypto Alerts\", TEXT)\n",
        "\n",
        "        # sending the mail\n",
        "        for receiver_email in receiver_list:\n",
        "          s.sendmail(sender_email, receiver_email, output_message)\n",
        "\n",
        "        # terminating the session\n",
        "        s.quit()\n",
        "\n",
        "    ############################################################################"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgr3chJWMjoF"
      },
      "source": [
        "# Establish Credentials \n",
        "sender_email = 'cryptoalerts167@gmail.com'\n",
        "sender_password = 'Kaboom001$$@'\n",
        "receiver_list = [ \"harshghodkar@gmail.com\"]\n",
        "Alerts(sender_email, sender_password, receiver_list)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GIfw-2_Mjld"
      },
      "source": [
        "def Common_words():\n",
        "    \"\"\"\n",
        "    Get a list of common vocab words in english to eliminte common vocab \n",
        "    similar to stock names from our list. \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List\n",
        "    \"\"\"\n",
        "    ############################# Retrieving Data ##############################\n",
        "\n",
        "    page = requests.get('https://www.ef.com/ca/english-resources/english-vocabulary/top-3000-words/')\n",
        "    word_html = BeautifulSoup(page.text,'html.parser').find_all('p')[11]\n",
        "    \n",
        "    ############################# Parsing Data #################################\n",
        "    \n",
        "    counter = 0\n",
        "    words = []\n",
        "    for item in word_html:\n",
        "      if counter%2 == 0:\n",
        "        words.append(item)\n",
        "      counter+=1\n",
        "    \n",
        "    ############################################################################\n",
        "    \n",
        "    return words "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG_6UkLmMjit"
      },
      "source": [
        "def crypto_liquidity(crypto_data):\n",
        "    \"\"\"\n",
        "    Get the current liquidity of all the stocks in the market. To actually analyze\n",
        "    the crypto that has a steady demand in the market.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    crypto_data : pd.DataFrame \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    ############################# Retrieving Data ##############################\n",
        "    \n",
        "    page = requests.get('https://coinmarketcap.com/exchanges/digifinex/')\n",
        "    soup = BeautifulSoup(page.text, 'lxml')\n",
        "    review = soup.find_all(class_='cmc-table-row')\n",
        "\n",
        "    ############################### Parsing Data ###############################\n",
        "    \n",
        "    data = []\n",
        "    for item in review:\n",
        "      # Using Regex for parsing \n",
        "      regex = re.compile('[0-9]+')\n",
        "      regex1 = re.compile('[A-Z][a-z]+')\n",
        "      regex2 = re.compile('%[0-9]+')\n",
        "      str_0 = item.text\n",
        "\n",
        "      try:\n",
        "        name = regex1.findall(str_0)[0]\n",
        "      except:\n",
        "        name = \"\"\n",
        "      try:\n",
        "        liquidity = regex2.findall(str_0)[0]\n",
        "      except:\n",
        "        liquidity = -1\n",
        "      \n",
        "      try:\n",
        "        data.append({'Name': name, 'Liquidity': int(liquidity[1:])})\n",
        "      except:\n",
        "        data.append({'Name': name, 'Liquidity': liquidity})\n",
        "\n",
        "    ########################### Merge Data Frames ##############################\n",
        "\n",
        "    data = pd.DataFrame(data)\n",
        "    data = data[data.Liquidity > 0]\n",
        "    data = data.groupby('Name', group_keys=False).apply(lambda x: x.loc[x.Liquidity.idxmax()])\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    data = pd.merge(left=crypto_data, right=data, how='left', left_on='Name', right_on='Name')\n",
        "    data = data[data.Liquidity.notnull()]\n",
        "    \n",
        "    ############################################################################\n",
        "\n",
        "    return data"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMEAQ-OdMjgE"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL30JlvJMjat"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w79fLwQHMRRU"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    }
  ]
}