# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XC-yPPbt17GW74IfX69i8aLj5dlCHHXH
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torchvision
import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split
from torchtext.legacy import data
from torchtext.legacy import datasets
import random
from torch.utils.data import DataLoader, TensorDataset, random_split

# %matplotlib inline

# Use a white background for matplotlib figures
matplotlib.rcParams['figure.facecolor'] = '#ffffff'

def generate_bigrams(x):
    n_grams = set(zip(*[x[i:] for i in range(2)]))
    for n_gram in n_grams:
        x.append(' '.join(n_gram))
    return x

SEED = 1234

torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

TEXT = data.Field(tokenize = 'spacy',
                  tokenizer_language = 'en_core_web_sm',
                  preprocessing = generate_bigrams)

LABEL = data.LabelField(dtype = torch.float)

train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)

train_data, valid_data = train_data.split(random_state = random.seed(SEED))

print("train_data: ",len(train_data), " test_data: ",len(test_data))

MAX_VOCAB_SIZE = 25_000

TEXT.build_vocab(train_data, 
                 max_size = MAX_VOCAB_SIZE, 
                 vectors = "glove.6B.100d", 
                 unk_init = torch.Tensor.normal_)

LABEL.build_vocab(train_data, )

BATCH_SIZE = 64

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(
    (train_data, valid_data, test_data), 
    batch_size = BATCH_SIZE, 
    device = device)

class Sentiment(nn.Module):
    def __init__(self, in_size, out_size, vocab_size, pad_idx):
      super().__init__()
      self.embedding = nn.Embedding(vocab_size, in_size, padding_idx=pad_idx)
      self.linear = nn.Linear(in_size, out_size)

    def forward(self, xb):
      emd = self.embedding(xb)
      emd = emd.permute(1, 0, 2)
      out = F.avg_pool2d(emd, (emd.shape[1], 1)).squeeze(1)
      return self.linear(out)

    def training_step(self, batch):
        TEXT, LABEL = batch 
        out = self(TEXT)                  # Generate predictions
        loss = F.cross_entropy(out, LABEL) # Calculate loss
        return loss

    def validation_step(self, batch):
        TEXT, LABEL = batch 
        out = self(TEXT)                    # Generate predictions
        loss = F.cross_entropy(out, LABEL)   # Calculate loss
        acc = accuracy(out, LABEL)           # Calculate accuracy
        return {'val_loss': loss, 'val_acc': acc}
        
    def validation_epoch_end(self, outputs):
        batch_losses = [x['val_loss'] for x in outputs]
        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses
        batch_accs = [x['val_acc'] for x in outputs]
        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies
        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}
    
    def epoch_end(self, epoch, result):
        print("Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}".format(epoch, result['val_loss'], result['val_acc']))

def evaluate(model, iterator, criterion):
    
    epoch_loss = 0
    epoch_acc = 0
    
    model.eval()
    
    with torch.no_grad():
    
        for batch in iterator:

            predictions = model(batch.text).squeeze(1)
            
            loss = criterion(predictions, batch.label)
            
            acc = binary_accuracy(predictions, batch.label)

            epoch_loss += loss.item()
            epoch_acc += acc.item()
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def train(model, iterator, criterion = nn.BCEWithLogitsLoss(), optimizer = torch.optim.Adam(model.parameters())):
    """
    Criterion
    There are a few alternatives to BCEWITHLogitLoss() such as using Sigmoid layer and BCE loss function
    seprately but when we combine the them togather in BCEWithLofitsLoss() we can achieve better results.

    Optimizer
    The Key difference between torch.optim.SGD() and torch.optim.Adam() is optim.Adam works relatively 
    on the flip side optim.SGD() follows certain parameter for changing the weights and bias constantly.

    Parameters
    ----------
    model     : ML Model 
    iterator  : BucketIterator (Data is Broken in small batches of torches)
    criterion : nn.BCEWithLogitsLoss 
    optimizer : either of torch.optim.Adam() or torch.optim.SGD()

    Returns
    -------
    float, float
    """
    epoch_loss = 0
    epoch_acc = 0
    
    model.train()
    
    for batch in iterator:    
        optimizer.zero_grad()
        predictions = model(batch.text).squeeze(1)
  
        loss = criterion(predictions, batch.label)
        acc = accuracy(predictions, batch.label)
        
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()

    # Return loss and accuracy resp
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

model = Sentiment(64, 1, len(TEXT.vocab), TEXT.vocab.stoi[TEXT.pad_token])

def accuracy(preds, y):
    """
    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8
    """

    #round predictions to the closest integer
    rounded_preds = torch.round(torch.sigmoid(preds))
    correct = (rounded_preds == y).float()
    acc = correct.sum() / len(correct)
    return acc

for batch in train_iterator:
  print(model(batch.text).squeeze(1))
  print(batch.text)
  break

N_EPOCHS = 5
min_loss = float('inf')
for epoch in range(N_EPOCHS):
    train_loss, train_acc = train(model, train_iterator)
    valid_loss, valid_acc = evaluate(model, valid_iterator)
    
    if valid_loss < min_loss:
        min_loss = valid_loss
        torch.save(model.state_dict(), 'tut3-model.pt')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
    print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')

import spacy
nlp = spacy.load('en_core_web_sm')

def predict_sentiment(model, sentence):
    model.eval()
    tokenized = generate_bigrams([tok.text for tok in nlp.tokenizer(sentence)])
    indexed = [TEXT.vocab.stoi[t] for t in tokenized]
    tensor = torch.LongTensor(indexed).to(device)
    tensor = tensor.unsqueeze(1)
    prediction = torch.sigmoid(model(tensor))
    return prediction.item()

predict_sentiment(model, 'I am so unsure about this stock')